ypred = predict(svm.best.overlapping, overlapping)
table("Predicted Values" = ypred, "True Values" = overlapping[, "y"])
set.seed(0)
nonlinear = data.frame(x1, x2, y)
plot(nonlinear$x1, nonlinear$x2, col = nonlinear$y)
#A linear kernel will fail in this scenario. Let's try using a radial kernel.
svm.radial = svm(y ~ .,
data = nonlinear,
kernel = "radial",
cost = seq(.75, 1.25, length = 5),
gamma = seq(.55, .95, length = 5)) #Default is 1/p.
#Visualizing the results of the support vector machine
plot(svm.radial, nonlinear)
summary(svm.radial)
svm.radial$index
#Inspecting the best model.
best.nonlinear.model = cv.svm.radial$best.model
#Inspecting the best model.
best.nonlinear.model = svm.radial$best.model
summary(best.nonlinear.model)
#Creating training and test sets.
#Splitting the data into training and test sets by an 70% - 30% split.
set.seed(0)
train.index = sample(1:nrow(wine), 7*nrow(wine)/10) #Training indices.
wine.test = wine[-train, ] #Test dataset.
quality.test = wine$quality[-train] #Test response.
wine = wine_
library(dplyr)
qual_fact = ifelse(wine$quality <= 5, "Low", "High")
matrx = data.matrix(wine)
matrx = scale(matrx, center = TRUE, scale = TRUE)
wine = data.frame(matrx)
wine = mutate(wine, quality = qual_fact)
#Creating training and test sets.
#Splitting the data into training and test sets by an 70% - 30% split.
set.seed(0)
train.index = sample(1:nrow(wine), 7*nrow(wine)/10) #Training indices.
wine.test = wine[-train, ] #Test dataset.
quality.test = wine$quality[-train] #Test response.
#fitting a support vector classifier by reducing the cost of a misclassified
#observation.
library(e1071)
x1 = wine$density
x2 = wine$alcohol
y = wine$quality
overlapping = data.frame(x1, x2, y)
plot(overlapping$x1, overlapping$x2, col = overlapping$y)
#Implement cross-validation to select the best parameter value of the cost.
set.seed(0)
cv.svm.overlapping = tune(svm,
y ~ .,
data = overlapping[train.index, ],
kernel = "linear",
ranges = list(cost = 10^(seq(-5, .5, length = 100))))
#Inspecting the cross-validation output.
summary(cv.svm.overlapping)
#Plotting the cross-validation results.
plot(cv.svm.overlapping$performances$cost,
cv.svm.overlapping$performances$error,
xlab = "Cost",
ylab = "Error Rate",
type = "l")
#Inspecting the best model.
best.overlapping.model = cv.svm.overlapping$best.model
summary(best.overlapping.model)
#Using the best model to predict the test data.
ypred = predict(best.overlapping.model, overlapping[test.index, ])
table("Predicted Values" = ypred, "True Values" = overlapping[test.index, "y"])
#Constructing and visualizing the final model.
svm.best.overlapping = svm(y ~ .,
data = overlapping,
kernel = "linear",
cost = best.overlapping.model$cost)
plot(svm.best.overlapping, overlapping)
summary(svm.best.overlapping)
svm.best.overlapping$index
ypred = predict(svm.best.overlapping, overlapping)
table("Predicted Values" = ypred, "True Values" = overlapping[, "y"])
set.seed(0)
nonlinear = data.frame(x1, x2, y)
plot(nonlinear$x1, nonlinear$x2, col = nonlinear$y)
#A linear kernel will fail in this scenario. Let's try using a radial kernel.
svm.radial = svm(y ~ .,
data = nonlinear[train.index, ],
kernel = "radial",
cost = seq(.75, 1.25, length = 5),
gamma = seq(.55, .95, length = 5)) #Default is 1/p.
#Visualizing the results of the support vector machine
plot(svm.radial, nonlinear)
summary(svm.radial)
svm.radial$index
#Inspecting the best model.
best.nonlinear.model = svm.radial$best.model
summary(best.nonlinear.model)
#Inspecting the best model.
best.nonlinear.model = svm.radial$best.model
#summary(best.nonlinear.model)
sapply(train, sd)
#####################################################
############ Multi Linear Regression ################
#####################################################
train = read.csv('./data/train_clean_std_full.csv')
#Basic numerical EDA for states dataset.
summary(train)
sapply(train, sd)
cor(train)
#Basic graphical EDA for the train dataset.
plot(train)
train = read.csv('./data/train_clean_std_full.csv')
setwd("~/NYCDSA/Projects/ML_Project")
train = read.csv('./data/train_clean_std_full.csv')
View(train)
summary(train)
sapply(train, sd)
cor(train)
corr_mtrx = cor(train)
View(corr_mtrx)
library(corrplot)
library(dplyr)
corr_mtrx = cor(train)
corrplot(corr, method="circle")
corr = cor(train)
corrplot(corr, method="circle")
train = read.csv('./data/train_clean_std_full.csv')
library(corrplot)
library(dplyr)
plot(train)
#Creating a saturated model (a model with all variables included).
model.saturated = lm(SalePrice ~ ., data = train)
summary(model.saturated) #Many predictor variables are not significant, yet the
#overall regression is significant.
plot(model.saturated) #Assessing the assumptions of the model.
vif(model.saturated) #Assessing the variance inflation factors for the variables
#in our model.
library(Hmisc)
vif(model.saturated) #Assessing the variance inflation factors for the variables
library(car) #Companion to applied regression.
vif(model.saturated) #Assessing the variance inflation factors for the variables
vif(model.saturated) #Assessing the variance inflation factors for the variables
#in our model.
library(VIF)
VIF(model.saturated)
######################################################
######################################################
#####[04] Multiple Linear Regression Lecture Code#####
######################################################
######################################################
#####################################################
#####Example using the State Information Dataset#####
#####################################################
help(state.x77)
state.x77 #Investigating the state.x77 dataset.
states = as.data.frame(state.x77) #Forcing the state.x77 dataset to be a dataframe.
#Cleaning up the column names so that there are no spaces.
colnames(states)[4] = "Life.Exp"
colnames(states)[6] = "HS.Grad"
#Creating a population density variable.
states[,9] = (states$Population*1000)/states$Area
colnames(states)[9] = "Density"
#Basic numerical EDA for states dataset.
summary(states)
sapply(states, sd)
cor(states)
#Basic graphical EDA for the states dataset.
plot(states)
#Can we estimate an individual's life expectancy based upon the state in which
#they reside?
#Creating a saturated model (a model with all variables included).
model.saturated = lm(Life.Exp ~ ., data = states)
summary(model.saturated) #Many predictor variables are not significant, yet the
#overall regression is significant.
plot(model.saturated) #Assessing the assumptions of the model.
vif(model.saturated) #Assessing the variance inflation factors for the variables
#in our model.
library(corrplot)
library(dplyr)
#Basic numerical EDA for states dataset.
#summary(train)
#sapply(train, sd)
#corr = cor(train)
#Basic graphical EDA for the train dataset.
#plot(train)
#corrplot(corr, method="circle")
#Creating a saturated model (a model with all variables included).
model.saturated = lm(SalePrice ~ ., data = train)
summary(model.saturated) #Many predictor variables are not significant, yet the
#overall regression is significant.
plot(model.saturated) #Assessing the assumptions of the model.
vif(model.saturated) #Assessing the variance inflation factors for the variables
#in our model.
alias(model.saturated)
avPlots(model.saturated)
train = read.csv('./data/train_clean_std_full.csv')
library(corrplot)
library(dplyr)
library(car) #Companion to applied regression.
#through which to search:
model.empty = lm(SalePrice ~ 1, data = train) #The model with an intercept ONLY.
model.full = lm(SalePrice ~ ., data = train) #The model with ALL variables.
scope = list(lower = formula(model.empty), upper = formula(model.full))
library(MASS) #The Modern Applied Statistics library.
#Stepwise regression using AIC as the criteria (the penalty k = 2).
forwardAIC = step(model.empty, scope, direction = "forward", k = 2)
backwardAIC = step(model.full, scope, direction = "backward", k = 2)
bothAIC.empty = step(model.empty, scope, direction = "both", k = 2)
bothAIC.full = step(model.full, scope, direction = "both", k = 2)
#####################################################
############ Multi Linear Regression ################
#####################################################
train = read.csv('./data/train_clean_std_full.csv')
library(corrplot)
library(dplyr)
#Basic numerical EDA for states dataset.
#summary(train)
#sapply(train, sd)
#corr = cor(train)
#Basic graphical EDA for the train dataset.
#plot(train)
#corrplot(corr, method="circle")
#Creating a saturated model (a model with all variables included).
#model.saturated = lm(SalePrice ~ ., data = train)
summary(model.saturated) #Many predictor variables are not significant, yet the
#overall regression is significant.
#plot(model.saturated) #Assessing the assumptions of the model.
library(car) #Companion to applied regression.
influencePlot(model.saturated)
#alias(model.saturated)
#vif(model.saturated) #Assessing the variance inflation factors for the variables
#in our model.
#avPlots(model.saturated)
#We can use stepwise regression to help automate the variable selection process.
#Here we define the minimal model, the full model, and the scope of the models
#through which to search:
model.empty = lm(SalePrice ~ 1, data = train) #The model with an intercept ONLY.
model.full = lm(SalePrice ~ ., data = train) #The model with ALL variables.
scope = list(lower = formula(model.empty), upper = formula(model.full))
library(MASS) #The Modern Applied Statistics library.
#Stepwise regression using AIC as the criteria (the penalty k = 2).
forwardAIC = step(model.empty, scope, direction = "forward", k = 2)
#####################################################
############ Multi Linear Regression ################
#####################################################
train = read.csv('./data/train_clean_std_full.csv')
library(corrplot)
library(dplyr)
#Basic numerical EDA for states dataset.
#summary(train)
#sapply(train, sd)
#corr = cor(train)
#Basic graphical EDA for the train dataset.
#plot(train)
#corrplot(corr, method="circle")
#Creating a saturated model (a model with all variables included).
#model.saturated = lm(SalePrice ~ ., data = train)
summary(model.saturated) #Many predictor variables are not significant, yet the
#overall regression is significant.
#plot(model.saturated) #Assessing the assumptions of the model.
library(car) #Companion to applied regression.
influencePlot(model.saturated)
#alias(model.saturated)
#vif(model.saturated) #Assessing the variance inflation factors for the variables
#in our model.
#avPlots(model.saturated)
#We can use stepwise regression to help automate the variable selection process.
#Here we define the minimal model, the full model, and the scope of the models
#through which to search:
model.empty = lm(SalePrice ~ 1, data = train) #The model with an intercept ONLY.
model.full = lm(SalePrice ~ ., data = train) #The model with ALL variables.
scope = list(lower = formula(model.empty), upper = formula(model.full))
library(MASS) #The Modern Applied Statistics library.
#Stepwise regression using AIC as the criteria (the penalty k = 2).
forwardAIC = step(model.empty, scope, direction = "forward",k = 2)
summary(forwardAIC)
plot(forwardAIC)
influencePlot(forwardAIC)
vif(forwardAIC)
avPlots(forwardAIC)
confint(forwardAIC)
#Predicting new observations.
forwardAIC$fitted.values #Returns the fitted values.
test = read.csv('./data/test_clean_std_full.csv')
predict(forwardAIC, test, interval = "confidence") #Construct confidence intervals
write.csv(predict_test, file = "forward_submission.csv")
predict_test = predict(forwardAIC, test, interval = "confidence") #Construct confidence intervals
write.csv(predict_test, file = "forward_submission.csv")
#through which to search:
model.empty = lm(SalePrice ~ 1, data = train) #The model with an intercept ONLY.
model.full = lm(SalePrice ~ ., data = train) #The model with ALL variables.
scope = list(lower = formula(model.empty), upper = formula(model.full))
library(MASS) #The Modern Applied Statistics library.
#Stepwise regression using AIC as the criteria (the penalty k = 2).
#forwardAIC = step(model.empty, scope, direction = "forward",k = 2)
#backwardAIC = step(model.full, scope, direction = "backward", k = 2)
#bothAIC.empty = step(model.empty, scope, direction = "both", k = 2)
bothAIC.full = step(model.full, scope, direction = "both", k = 2)
model.empty = lm(SalePrice ~ 1, data = train) #The model with an intercept ONLY.
model.full = lm(SalePrice ~ ., data = train) #The model with ALL variables.
scope = list(lower = formula(model.empty), upper = formula(model.full))
library(MASS) #The Modern Applied Statistics library.
#Stepwise regression using AIC as the criteria (the penalty k = 2).
#forwardAIC = step(model.empty, scope, direction = "forward",k = 2)
#backwardAIC = step(model.full, scope, direction = "backward", k = 2)
#bothAIC.empty = step(model.empty, scope, direction = "both", k = 2)
bothAIC.full = step(model.full, scope, direction = "both", k = 2)
#####################################################
############ Multi Linear Regression ################
#####################################################
train = read.csv('./data/train_clean_std_full.csv')
library(corrplot)
library(dplyr)
#Basic numerical EDA for states dataset.
#summary(train)
#sapply(train, sd)
#corr = cor(train)
#Basic graphical EDA for the train dataset.
#plot(train)
#corrplot(corr, method="circle")
#Creating a saturated model (a model with all variables included).
#model.saturated = lm(SalePrice ~ ., data = train)
summary(model.saturated) #Many predictor variables are not significant, yet the
#overall regression is significant.
#plot(model.saturated) #Assessing the assumptions of the model.
library(car) #Companion to applied regression.
influencePlot(model.saturated)
#alias(model.saturated)
#vif(model.saturated) #Assessing the variance inflation factors for the variables
#in our model.
#avPlots(model.saturated)
#We can use stepwise regression to help automate the variable selection process.
#Here we define the minimal model, the full model, and the scope of the models
#through which to search:
model.empty = lm(SalePrice ~ 1, data = train) #The model with an intercept ONLY.
model.full = lm(SalePrice ~ ., data = train) #The model with ALL variables.
scope = list(lower = formula(model.empty), upper = formula(model.full))
library(MASS) #The Modern Applied Statistics library.
#Stepwise regression using AIC as the criteria (the penalty k = 2).
#forwardAIC = step(model.empty, scope, direction = "forward",k = 2)
#backwardAIC = step(model.full, scope, direction = "backward", k = 2)
#bothAIC.empty = step(model.empty, scope, direction = "both", k = 2)
bothAIC.full = step(model.full, scope, direction = "both", k = 2)
#####################################################
############ Multi Linear Regression ################
#####################################################
train = read.csv('./data/train_clean_std_full.csv')
library(corrplot)
library(dplyr)
#Basic numerical EDA for states dataset.
#summary(train)
#sapply(train, sd)
#corr = cor(train)
#Basic graphical EDA for the train dataset.
#plot(train)
#corrplot(corr, method="circle")
#Creating a saturated model (a model with all variables included).
#model.saturated = lm(SalePrice ~ ., data = train)
summary(model.saturated) #Many predictor variables are not significant, yet the
#overall regression is significant.
#plot(model.saturated) #Assessing the assumptions of the model.
library(car) #Companion to applied regression.
#influencePlot(model.saturated)
#alias(model.saturated)
#vif(model.saturated) #Assessing the variance inflation factors for the variables
#in our model.
#avPlots(model.saturated)
#We can use stepwise regression to help automate the variable selection process.
#Here we define the minimal model, the full model, and the scope of the models
#through which to search:
model.empty = lm(SalePrice ~ 1, data = train) #The model with an intercept ONLY.
model.full = lm(SalePrice ~ ., data = train) #The model with ALL variables.
scope = list(lower = formula(model.empty), upper = formula(model.full))
library(MASS) #The Modern Applied Statistics library.
#Stepwise regression using AIC as the criteria (the penalty k = 2).
#forwardAIC = step(model.empty, scope, direction = "forward",k = 2)
#backwardAIC = step(model.full, scope, direction = "backward", k = 2)
#bothAIC.empty = step(model.empty, scope, direction = "both", k = 2)
bothAIC.full = step(model.full, scope, direction = "both", k = 2)
summary(bothAIC.full)
test = read.csv('./data/test_clean_std_full.csv')
predict_test1 = predict(bothAIC.full, test, interval = "confidence") #Construct confidence intervals
predict_test2 = predict(bothAIC.full, test, interval = "prediction") #Construct confidence intervals
View(predict_test1)
View(predict_test2)
predict_test = predict(bothAIC.full, test, interval = "confidence") #Construct confidence intervals
write.csv(predict_test, file = "both_backward_submission.csv")
#####################################################
############ Multi Linear Regression ################
#####################################################
train = read.csv('./data/train_clean_std_full.csv')
library(corrplot)
library(dplyr)
#Basic numerical EDA for states dataset.
#summary(train)
#sapply(train, sd)
#corr = cor(train)
#Basic graphical EDA for the train dataset.
#plot(train)
#corrplot(corr, method="circle")
#Creating a saturated model (a model with all variables included).
model.saturated = lm(SalePrice ~ ., data = train)
summary(model.saturated) #Many predictor variables are not significant, yet the
#overall regression is significant.
#plot(model.saturated) #Assessing the assumptions of the model.
library(car) #Companion to applied regression.
influencePlot(model.saturated)
#####################################################
############ Multi Linear Regression ################
#####################################################
train = read.csv('./data/train_clean_std_full.csv')
library(corrplot)
library(dplyr)
#Basic numerical EDA for states dataset.
#summary(train)
#sapply(train, sd)
#corr = cor(train)
#Basic graphical EDA for the train dataset.
#plot(train)
#corrplot(corr, method="circle")
#Creating a saturated model (a model with all variables included).
model.saturated = lm(SalePrice ~ ., data = train)
summary(model.saturated) #Many predictor variables are not significant, yet the
#overall regression is significant.
plot(model.saturated) #Assessing the assumptions of the model.
#####################################################
############ Multi Linear Regression ################
#####################################################
train = read.csv('./data/train_clean_std_full.csv')
library(corrplot)
library(dplyr)
#Basic numerical EDA for states dataset.
#summary(train)
#sapply(train, sd)
#corr = cor(train)
#Basic graphical EDA for the train dataset.
#plot(train)
#corrplot(corr, method="circle")
#Creating a saturated model (a model with all variables included).
model.saturated = lm(SalePrice ~ ., data = train)
summary(model.saturated) #Many predictor variables are not significant, yet the
#overall regression is significant.
plot(model.saturated) #Assessing the assumptions of the model.
############ Multi Linear Regression ################
#####################################################
train = read.csv('./data/train_clean_std_full.csv')
library(corrplot)
library(dplyr)
#Basic numerical EDA for states dataset.
#summary(train)
#sapply(train, sd)
#corr = cor(train)
#Basic graphical EDA for the train dataset.
#plot(train)
#corrplot(corr, method="circle")
#Creating a saturated model (a model with all variables included).
model.saturated = lm(SalePrice ~ ., data = train)
#summary(model.saturated) #Many predictor variables are not significant, yet the
#overall regression is significant.
plot(model.saturated) #Assessing the assumptions of the model.
#####################################################
train = read.csv('./data/train_clean_std_full.csv')
library(corrplot)
library(dplyr)
#Basic numerical EDA for states dataset.
#summary(train)
#sapply(train, sd)
#corr = cor(train)
#Basic graphical EDA for the train dataset.
#plot(train)
#corrplot(corr, method="circle")
#Creating a saturated model (a model with all variables included).
model.saturated = lm(SalePrice ~ ., data = train)
#summary(model.saturated) #Many predictor variables are not significant, yet the
#overall regression is significant.
plot(model.saturated) #Assessing the assumptions of the model.
#####################################################
train = read.csv('./data/train_clean_std_full.csv')
setwd("~/NYCDSA/Projects/ML_Project/JF/data")
#####################################################
############ Multi Linear Regression ################
#####################################################
train = read.csv('train_clean_std_full.csv')
library(corrplot)
library(dplyr)
#Basic numerical EDA for states dataset.
#summary(train)
#sapply(train, sd)
#corr = cor(train)
#Basic graphical EDA for the train dataset.
#plot(train)
#corrplot(corr, method="circle")
#Creating a saturated model (a model with all variables included).
model.saturated = lm(SalePrice ~ ., data = train)
#summary(model.saturated) #Many predictor variables are not significant, yet the
#overall regression is significant.
plot(model.saturated) #Assessing the assumptions of the model.
#####################################################
############ Multi Linear Regression ################
#####################################################
train = read.csv('train_clean_std_full.csv')
library(corrplot)
library(dplyr)
#Basic numerical EDA for states dataset.
#summary(train)
#sapply(train, sd)
#corr = cor(train)
#Basic graphical EDA for the train dataset.
#plot(train)
#corrplot(corr, method="circle")
#Creating a saturated model (a model with all variables included).
model.saturated = lm(SalePrice ~ ., data = train)
#summary(model.saturated) #Many predictor variables are not significant, yet the
#overall regression is significant.
plot(model.saturated) #Assessing the assumptions of the model.
